{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T03:53:31.940766Z",
     "start_time": "2019-07-07T03:53:31.935868Z"
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T22:34:18.768587Z",
     "start_time": "2019-07-15T22:34:18.725225Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, re, sys, pickle, datetime\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA,NMF\n",
    "from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor,GradientBoostingRegressor\n",
    "from sklearn.feature_selection import SelectKBest,f_regression,mutual_info_regression\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import LogisticRegression,Lasso,LinearRegression,Ridge,ElasticNetCV,ElasticNet,Lars,LassoCV,RidgeCV,LarsCV,LassoLarsCV,LassoLarsIC,OrthogonalMatchingPursuitCV,OrthogonalMatchingPursuit\n",
    "from sklearn.manifold import TSNE,MDS\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix,f1_score\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,RepeatedKFold,LeaveOneOut,cross_val_score,cross_validate\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier,MLPRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,PolynomialFeatures\n",
    "from sklearn.svm import LinearSVC,SVR\n",
    "from sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor\n",
    "#from sklearn import tree\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import multiprocessing\n",
    "nproc = max([1,multiprocessing.cpu_count()-2])\n",
    "from joblib import Parallel,delayed\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import loo_q2 as loo\n",
    "\n",
    "randomstate = 42\n",
    "\n",
    "def plot_fit(y_train,y_pred_train,y_test,y_pred_test,leg=True,sav=False,label=\"y\",loo_pred=[]):\n",
    "    y_orig_min = np.min(np.hstack((y_train,y_test)))\n",
    "    y_pred_min = np.min(np.hstack((y_pred_train,y_pred_test)))\n",
    "    y_orig_max = np.max(np.hstack((y_train,y_test)))\n",
    "    y_pred_max = np.max(np.hstack((y_pred_train,y_pred_test)))\n",
    "    delta_x = 0.04 * (y_orig_max-y_orig_min)\n",
    "    delta_y = 0.04 * (y_pred_max-y_pred_min)\n",
    "           \n",
    "    yy_fit = np.polyfit(y_train,y_pred_train,deg=1)\n",
    "    yy_fit_line = yy_fit[1]+yy_fit[0]*y_train\n",
    "    \n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.plot(np.linspace(y_orig_min-delta_x,y_orig_max+delta_x), np.linspace(y_orig_min-delta_x,y_orig_max+delta_x),color=\"grey\")\n",
    "    plt.xlim([y_orig_min-delta_x,y_orig_max+delta_x])\n",
    "    plt.ylim([y_pred_min-delta_y,y_pred_max+delta_y])\n",
    "    if len(loo_pred) != 0:\n",
    "        plt.scatter(y_train,loo_train,label=\"LOO\",color=\"black\",marker=\"s\",facecolor='none')\n",
    "    plt.scatter(y_train,y_pred_train,label=\"training\",color=\"black\",marker=\"s\") # ,alpha=0.6\n",
    "    plt.scatter(y_test,y_pred_test,label=\"test\",color=\"red\",marker=\"s\")     #,alpha=0.25\n",
    "    plt.plot(y_train,yy_fit_line,color=\"black\") #,alpha=0.2\n",
    "    if leg:\n",
    "        plt.legend(loc='lower right')\n",
    "    plt.xlabel(label+\" measured\",fontsize=20)\n",
    "    plt.ylabel(label+\" predicted\",fontsize=20)\n",
    "    \n",
    "#     plt.xticks([])\n",
    "#     plt.yticks([])\n",
    "    \n",
    "    if not sav:\n",
    "        plt.show()  \n",
    "    else:\n",
    "        plt.savefig(sav)\n",
    "        \n",
    "def r2_val(y_test,y_pred_test,y_train):\n",
    "    \"\"\"Calculates the external R2 pred as described:\n",
    "    https://pdfs.semanticscholar.org/4eb2/5ff5a87f2fd6789c5b9954eddddfd1c59dab.pdf\"\"\"\n",
    "    y_resid = y_pred_test - y_test\n",
    "    SS_resid = np.sum(y_resid**2)\n",
    "    y_var = y_test - np.mean(y_train)\n",
    "    SS_total = np.sum(y_var**2)\n",
    "    r2_validation = 1-SS_resid/SS_total\n",
    "    return(r2_validation)\n",
    "\n",
    "def repeated_k_fold(X_train,y_train,reg = LinearRegression(), k=3, n=100):\n",
    "    \"\"\"Reapeated k-fold cross-validation. \n",
    "    For each of n repeats, the (training)data is split into k folds. \n",
    "    For each fold, this part of the data is predicted using the rest. \n",
    "    Once this is done for all k folds, the coefficient of determination (R^2) of the predictions of all folds combined (= the complete data set) is evaluated\n",
    "    This is repeated n times and all n R^2 are returned for averaging/further analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    rkf = RepeatedKFold(n_splits=k, n_repeats=n)\n",
    "    r2_scores = []\n",
    "    y_validations,y_predictions = np.zeros((np.shape(X_train)[0],n)),np.zeros((np.shape(X_train)[0],n))\n",
    "    foldcount = 0\n",
    "    for i,foldsplit in enumerate(rkf.split(X_train)):\n",
    "        fold, rep = i%k, int(i/k) # Which of k folds. Which of n repeats\n",
    "        model = reg.fit(X_train[foldsplit[0]],y_train[foldsplit[0]]) # foldsplit[0]: k-1 training folds\n",
    "        y_validations[foldcount:foldcount+len(foldsplit[1]),rep] = y_train[foldsplit[1]] # foldsplit[1]: validation fold\n",
    "        y_predictions[foldcount:foldcount+len(foldsplit[1]),rep]  = model.predict(X_train[foldsplit[1]])\n",
    "        foldcount += len(foldsplit[1])\n",
    "        if fold+1==k:\n",
    "            foldcount = 0\n",
    "    r2_scores = np.asarray([metrics.r2_score(y_validations[:,rep],y_predictions[:,rep]) for rep in range(n)])\n",
    "    return(r2_scores)\n",
    "\n",
    "# keepmodels = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T19:44:31.116959Z",
     "start_time": "2019-07-15T19:44:31.011638Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# all data in a single file\n",
    "excel_file = \"190415_P_ligands_base_overview\"\n",
    "excel_sheet = \"Tabelle2\"\n",
    "num_par = 141\n",
    "par_start_col = 3 # 0-indexed\n",
    "num_samples = 23\n",
    "response_col = 2 # 0-indexed\n",
    "y_label_col = 0 # 0-indexed\n",
    "\n",
    "apply_mask = True # remove samples with empty response\n",
    "verbose = True\n",
    "xlabelrow = True\n",
    "\n",
    "inp = pd.read_excel(excel_file+\".xlsx\",excel_sheet,header=0,index_col=y_label_col,nrows=num_samples+int(xlabelrow),\n",
    "                    usecols=max(y_label_col,response_col,num_par+par_start_col)-1)\n",
    "if xlabelrow:\n",
    "    X_names = list(inp.iloc[0,par_start_col-1:num_par+par_start_col-1])\n",
    "    X_labels = list(inp.columns)[par_start_col-1:num_par+par_start_col-1]\n",
    "    resp_label = list(inp.columns)[response_col-1]\n",
    "    inp.drop(index=inp.index[0],inplace=True)\n",
    "else:\n",
    "    X_labels = list(inp.columns)[par_start_col-1:num_par+par_start_col-1]\n",
    "    X_names = X_labels\n",
    "    resp_label = list(inp.columns)[response_col-1]\n",
    "\n",
    "X_labelname = [\" \".join(i) for i in zip(X_labels,X_names)] \n",
    "X_labelname_dict = dict(zip(X_labels,X_names))\n",
    "y = np.asarray(inp[resp_label],dtype=np.float)\n",
    "X = np.asarray(inp[X_labels],dtype=np.float)\n",
    "y_labels = np.asarray(list(inp.index),dtype=str)\n",
    "y_labels_comp= y_labels\n",
    "\n",
    "if apply_mask:\n",
    "    mask = y.nonzero()[0]\n",
    "    mask = ~np.isnan(y)\n",
    "    print(\"n_samples before removing empty cells: {}\".format(len(y)))\n",
    "    print(\"Removing {} samples.\".format(len(y)-sum(mask)))\n",
    "    X = X[np.array(mask)]\n",
    "    y = y[np.array(mask)]\n",
    "    y_labels = y_labels[np.array(mask)]\n",
    "X_all = X\n",
    "if verbose:\n",
    "    print(\"Shape X: {}\".format(X.shape))\n",
    "    print(\"Shape y: {}\".format(y.shape)) \n",
    "    print(\"Shape labels: {}\".format(y_labels.shape)) \n",
    "    print(\"First X cell: {}\".format(X[0,0]))\n",
    "    print(\"Last X cell:  {}\".format(X[-1,-1]))\n",
    "    print(\"First y: {}\".format(y[0]))\n",
    "    print(\"Last y:  {}\".format(y[-1]))\n",
    "    print(\"Last label: {}\".format(y_labels[-1]))\n",
    "    #print(inp.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T06:02:24.321808Z",
     "start_time": "2019-07-15T06:02:17.820488Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## separate files for exp data and comp data\n",
    "\n",
    "comp_file = \"Phosphine_library_DFT_props_190517\"\n",
    "comp_sheet = \"selprops_n_only\"\n",
    "num_par = 135\n",
    "par_start_col = 1 # 0-indexed\n",
    "comp_num_samples = 1198\n",
    "y_label_col_comp = 0 # 0-indexed\n",
    "\n",
    "exp_file = \"Phosphine_library_explitdata\"\n",
    "exp_sheet = \"LibraryGTZ\"\n",
    "exp_num_samples = 1282\n",
    "response_col = 7 # 0-indexed\n",
    "y_label_col_exp = 1 # 0-indexed\n",
    "\n",
    "compinp = pd.read_excel(comp_file+\".xlsx\",comp_sheet,header=0,index_col=y_label_col_comp,\n",
    "                        nrows=comp_num_samples+1,usecols=num_par+par_start_col-1)\n",
    "compinp.index = compinp.index.map(str)\n",
    "expinp = pd.read_excel(exp_file+\".xlsx\",exp_sheet,header=4,index_col=y_label_col_exp,\n",
    "                       nrows=exp_num_samples,usecols=response_col)\n",
    "expinp.index = [i.zfill(4) for i in expinp.index.map(str)]\n",
    "\n",
    "xlabelrow = True\n",
    "verbose = True\n",
    "\n",
    "X_names = list(compinp.iloc[0,par_start_col-1:num_par+par_start_col-1])\n",
    "X_labels = list(compinp.columns)[par_start_col-1:num_par+par_start_col-1]\n",
    "compinp.drop(index=compinp.index[0],inplace=True)\n",
    "X_all = np.asarray(compinp[X_labels],dtype=np.float)\n",
    "y_labels_comp = np.asarray(list(compinp.index),dtype=str)\n",
    "compnan = np.isnan(X_all).any(axis=1)\n",
    "y_labels_comp,X_all = y_labels_comp[~compnan],X_all[~compnan]\n",
    "\n",
    "X_labelname = [\" \".join(i) for i in zip(X_labels,X_names)] \n",
    "X_labelname_dict = dict(zip(X_labels,X_names))\n",
    "\n",
    "resp_label = list(expinp.columns)[response_col-1]\n",
    "y = np.asarray(expinp.iloc[:,response_col-1],dtype=np.float)\n",
    "y_labels_exp = np.asarray(list(expinp.index),dtype=str)\n",
    "\n",
    "mask_y = y.nonzero()[0]\n",
    "mask_y = ~np.isnan(y)\n",
    "mask_X = np.array([True if i in y_labels_comp else False for i in y_labels_exp])\n",
    "mask = mask_y&mask_X\n",
    "print(\"n_samples before removing empty cells: {}\".format(len(y)))\n",
    "print(\"Removing {} samples.\".format(len(y)-sum(mask)))\n",
    "y = y[np.array(mask)]\n",
    "y_labels = y_labels_exp[np.array(mask)]\n",
    "\n",
    "X = np.asarray(compinp.loc[y_labels],dtype=np.float)\n",
    "\n",
    "if verbose:\n",
    "    print(\"Shape X (all): {}\".format(X_all.shape))\n",
    "    print(\"Shape X (exp): {}\".format(X.shape))\n",
    "    print(\"Shape y (exp): {}\".format(y.shape)) \n",
    "    print(\"Shape labels (exp): {}\".format(y_labels.shape)) \n",
    "    print(\"First X (exp) cell: {}\".format(X[0,0]))\n",
    "    print(\"Last X (exp) cell:  {}\".format(X[-1,-1]))\n",
    "    print(\"First y: {}\".format(y[0]))\n",
    "    print(\"Last y:  {}\".format(y[-1]))\n",
    "    print(\"Last label exp: {}\".format(y_labels[-1]))\n",
    "    print(\"Last label comp: {}\".format(y_labels_comp[-3:]))\n",
    "    #print(inp.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Histograms and univariate correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T19:46:59.366805Z",
     "start_time": "2019-07-15T19:46:35.105184Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize Histograms and univariate correlations for all (or selected) features\n",
    "features = range(np.shape(X)[1])   # iterate over all features\n",
    "# examples for selecting features\n",
    "#specify names:\n",
    "# features = [x_names.index(\"sterimol_5-.5cB5_max\")]\n",
    "#specify x-numbers (1-indexed):\n",
    "# features_x = [\"x74\",\"x133\"]\n",
    "# features = [x_labels.index(i) for i in features_x]\n",
    "#specify ranges (0-indexed)\n",
    "# features = itertools.chain(range(75,85),range(90,95))\n",
    "\n",
    "for f_ind in features:\n",
    "    feature = X_labels[f_ind]\n",
    "    print(feature, X_names[f_ind])\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(X[:,f_ind], y)\n",
    "    fit_line = intercept+slope*X[:,f_ind]\n",
    "    \n",
    "    plt.figure(figsize=(9, 4))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.hist(X[:,f_ind], bins=\"auto\")\n",
    "    plt.ylabel(\"frequency\")\n",
    "    plt.xlabel(X_names[f_ind])\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.scatter(X[:,f_ind], y,color=\"black\",marker=\"s\",alpha=0.5)    \n",
    "    plt.plot(X[:,f_ind],fit_line,color=\"black\")\n",
    "    plt.xlabel(X_names[f_ind],fontsize=20)\n",
    "    plt.ylabel(\"y\",fontsize=20)\n",
    "\n",
    "#     plt.xticks(np.arange(round(min(X[:,f_ind])-0.005,3), round(max(X[:,f_ind])+0.005,3), 0.03),fontsize=15)\n",
    "    plt.yticks(fontsize=15)        \n",
    "    plt.tight_layout()\n",
    "    plt.show()    \n",
    "\n",
    "    if p_value > 0.01:\n",
    "        print(\"R^2 = {:.2f}; p-value = {:.2f}\".format(r_value**2,p_value))\n",
    "    else:\n",
    "        print(\"R^2 = {:.2f}; p-value = {:.2E}\".format(r_value**2,p_value))\n",
    "    print(\"\\n-------------------------------------------------------------------------------\\n\")\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot a feature vs. another feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T19:48:13.560985Z",
     "start_time": "2019-07-15T19:48:13.307993Z"
    }
   },
   "outputs": [],
   "source": [
    "#select two features to visualize\n",
    "# can be integer-index of features, or string with x-number\n",
    "f_ind_1 = \"x1\"\n",
    "f_ind_2 = \"x76\"\n",
    "\n",
    "if type(f_ind_1) == str:\n",
    "    [f_ind_1,f_ind_2] = [X_labels.index(i) for i in [f_ind_1,f_ind_2]]\n",
    "\n",
    "print(X_labels[f_ind_1], X_names[f_ind_1])\n",
    "print(X_labels[f_ind_2], X_names[f_ind_2])\n",
    "print(\"\\n{} samples\".format(np.shape(X[:,f_ind_1])[0]))\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(X[:,f_ind_1],X[:,f_ind_2])\n",
    "fit_line = intercept+slope*X[:,f_ind_1]\n",
    "print(\"R^2 = {:.2f}; p-value = {:.2E}\".format(r_value**2,p_value))\n",
    "\n",
    "plt.figure(figsize=(13, 4))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.hist(X[:,f_ind_1], bins=\"auto\",color=\"black\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.xlabel(X_names[f_ind_1])\n",
    "plt.subplot(1,3,2)\n",
    "plt.hist(X[:,f_ind_2], bins=\"auto\",color=\"black\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.xlabel(X_names[f_ind_2])\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.scatter(X[:,f_ind_1], X[:,f_ind_2],color=\"black\",marker=\"s\")    \n",
    "#plt.plot(X[:,f_ind_1],fit_line)\n",
    "plt.xlabel(X_names[f_ind_1])\n",
    "plt.ylabel(X_names[f_ind_2])\n",
    "plt.tight_layout()\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T22:38:40.627583Z",
     "start_time": "2019-07-15T22:38:40.477853Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "# this will open an interactive plot that you can enlarge and zoom into\n",
    "\n",
    "corrmap = np.corrcoef(X.T)\n",
    "plt.matshow(corrmap)\n",
    "plt.xticks(range(len(X_labels)),X_labels, fontsize=10, rotation=90)\n",
    "plt.yticks(range(len(X_labels)),X_labels, fontsize=10)\n",
    "cb = plt.colorbar()\n",
    "cb.ax.tick_params(labelsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-node Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T19:49:28.780854Z",
     "start_time": "2019-07-15T19:49:28.654173Z"
    }
   },
   "outputs": [],
   "source": [
    "# import graphviz \n",
    "\n",
    "dt = DecisionTreeRegressor(max_depth=1).fit(X, y)\n",
    "print(\"Accuracy: {:.2f}\".format(dt.score(X, y)))\n",
    "\n",
    "feat = int(np.where(dt.feature_importances_ != 0)[0])\n",
    "print(X_labels[feat],X_names[feat])\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.scatter(X[:,feat], y)    \n",
    "plt.xlabel(X_names[feat])\n",
    "plt.ylabel(\"y\")\n",
    "\n",
    "# dot_data = tree.export_graphviz(dt, out_file=None, \n",
    "#                      feature_names=X_names,   \n",
    "#                      filled=True, rounded=True,  \n",
    "#                      special_characters=True)  \n",
    "# graph = graphviz.Source(dot_data)\n",
    "# graph  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify highest and lowest group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T19:50:01.422747Z",
     "start_time": "2019-07-15T19:50:01.182727Z"
    }
   },
   "outputs": [],
   "source": [
    "# find features that separate the group with highest/lowest output y\n",
    "# change the definition of bins in the histogram to get control over how these groups are defined\n",
    "y_hist,y_bin_edges = np.histogram(y,bins=\"auto\")\n",
    "y_class_low = [0 if i < y_bin_edges[1] else 1 for i in y]\n",
    "y_class_high = [1 if i > y_bin_edges[-2] else 0 for i in y]\n",
    "\n",
    "plt.figure(figsize=(8.5, 4))\n",
    "n_classes = 2\n",
    "plot_colors = \"rg\"\n",
    "plot_step = 0.02\n",
    "y_classes = [np.asarray(y_class_low),np.asarray(y_class_high)]\n",
    "for y_class,i in zip(y_classes,[1,2]):\n",
    "    dt = DecisionTreeClassifier(max_depth=1).fit(X, y_class)\n",
    "    feat = int(np.where(dt.feature_importances_ != 0)[0])    \n",
    "    a = (\"f1_score: {:.2f}\".format(metrics.f1_score(y_class,dt.predict(X))))\n",
    "#    b = (\"auc: {:.2f}\".format(metrics.roc_auc_score(y_class,dt.predict(X))))\n",
    "    print(X_labels[feat],X_names[feat])    \n",
    "    xpltlabel = X_labels[feat] + \"\\n\" + X_names[feat] + \"\\n\" + a# + \"\\n\" + b\n",
    "\n",
    "    dt_plt = DecisionTreeClassifier(max_depth=1).fit(X[:,feat].reshape(-1, 1), y_class)\n",
    "    x_min, x_max = X[:, feat].min(), X[:, feat].max()\n",
    "    y_min, y_max = y.min(), y.max()\n",
    "    dx,dy = x_max-x_min,y_max-y_min\n",
    "    xx, yy = np.meshgrid(np.arange(x_min-0.04*dx, x_max+0.04*dx, plot_step),\n",
    "                         np.arange(y_min-0.04*dy, y_max+0.04*dy, plot_step))\n",
    "    \n",
    "    plt.subplot(1,2,i)\n",
    "    plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
    "\n",
    "    Z = dt_plt.predict(xx.ravel().reshape(-1, 1))\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cs = plt.contourf(xx, yy, Z, cmap=\"Pastel1\")#plt.cm.RdYlBu)\n",
    "\n",
    "    plt.xlabel(xpltlabel)\n",
    "    plt.ylabel(\"y\")\n",
    "\n",
    "    for i, color in zip(range(n_classes), plot_colors):\n",
    "        idx = np.where(y_class == i)\n",
    "        plt.scatter(X[idx, feat], y[idx], c=color,cmap=plt.cm.RdYlBu, edgecolor='black', s=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# dot_data = tree.export_graphviz(dt_plt, out_file=None, \n",
    "#                      feature_names=[x_names[feat]],   \n",
    "#                      filled=True, rounded=True,  \n",
    "#                      special_characters=True)  \n",
    "# graph = graphviz.Source(dot_data)\n",
    "# graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T03:12:00.181835Z",
     "start_time": "2019-07-07T03:12:00.176867Z"
    }
   },
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T19:52:12.321856Z",
     "start_time": "2019-07-15T19:52:12.225817Z"
    }
   },
   "outputs": [],
   "source": [
    "# divide samples into two classes based on y_cut, find the feature that most clearly distinguishes these groups\n",
    "y_cut = 1\n",
    "\n",
    "#this can be done on a subset of features\n",
    "# features = [i for i in itertools.chain(range(75,85),range(90,95))]\n",
    "# features = [i for i in range(38,135)]\n",
    "features = range(np.shape(X)[1])\n",
    "X_use = X[:,features]\n",
    "\n",
    "y_class = np.array([0 if i < y_cut else 1 for i in y])\n",
    "n_classes=2\n",
    "dt = DecisionTreeClassifier(max_depth=1).fit(X_use, y_class)\n",
    "\n",
    "feat = features[int(np.where(dt.feature_importances_ != 0)[0])]\n",
    "print(X_labels[feat],X_names[feat])\n",
    "\n",
    "dt_plt = DecisionTreeClassifier(max_depth=1).fit(X[:,feat].reshape(-1, 1), y_class)\n",
    "print(\"Decision threshold = {:.2f}\\nAccuracy: {:.2f}\\nf1_score: {:.2f}\\nN = {}\".format(\n",
    "        dt_plt.tree_.threshold[0],\n",
    "        dt_plt.score(X[:,feat].reshape(-1, 1), y_class),\n",
    "        metrics.f1_score(y_class,dt_plt.predict(X[:,feat].reshape(-1, 1))),\n",
    "        len(y)\n",
    "    ))\n",
    "\n",
    "plot_colors = \"rg\"\n",
    "plot_step = 0.02\n",
    "x_min, x_max = X[:,feat].min(), X[:,feat].max()\n",
    "y_min, y_max = y.min(), y.max()\n",
    "dx,dy = x_max-x_min,y_max-y_min\n",
    "xx, yy = np.meshgrid(np.arange(x_min-0.04*dx, x_max+0.04*dx, plot_step),\n",
    "                     np.arange(y_min-0.04*dy, y_max+0.04*dy, plot_step))\n",
    "\n",
    "plt.figure(figsize=(4, 4))    \n",
    "plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
    "\n",
    "Z = dt_plt.predict(xx.ravel().reshape(-1, 1))\n",
    "Z = Z.reshape(xx.shape)\n",
    "cs = plt.contourf(xx, yy, Z, cmap=\"Pastel1\")#plt.cm.RdYlBu)\n",
    "xpltlabel = X_labels[feat] + \"\\n\" + X_names[feat]\n",
    "\n",
    "plt.xlabel(xpltlabel)\n",
    "plt.ylabel(\"y\")\n",
    "\n",
    "for i, color in zip(range(n_classes), plot_colors):\n",
    "    idx = np.where(y_class == i)\n",
    "    plt.scatter(X[idx, feat], y[idx], c=color,cmap=plt.cm.RdYlBu, edgecolor='black', s=15)    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Property threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T19:54:05.336181Z",
     "start_time": "2019-07-15T19:54:01.991737Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# divide samples into two classes based on y_cut, visualize how features separate these classes\n",
    "y_cut = 1\n",
    "\n",
    "#select features here\n",
    "# features = range(len(X_labels))   # iterate over all features\n",
    "features = itertools.chain(range(96,128))\n",
    "\n",
    "y_class = np.array([0 if i < y_cut else 1 for i in y])\n",
    "n_classes = 2\n",
    "plot_colors = \"rg\"\n",
    "plot_step = 0.02\n",
    "\n",
    "for f_ind in features:\n",
    "    feature = X_labels[f_ind]\n",
    "    print(feature, X_names[f_ind])\n",
    "    dt = DecisionTreeClassifier(max_depth=1).fit(X[:,f_ind].reshape(-1, 1), y_class)   \n",
    "    print(\"Decision threshold = {:.2f}\\nAccuracy: {:.2f}\\nf1_score: {:.2f}\\nN = {}\".format(\n",
    "        dt.tree_.threshold[0],\n",
    "        dt.score(X[:,f_ind].reshape(-1, 1), y_class),\n",
    "        metrics.f1_score(y_class,dt.predict(X[:,f_ind].reshape(-1, 1))),\n",
    "        len(y)\n",
    "    ))\n",
    "    \n",
    "    dt_plt = DecisionTreeClassifier(max_depth=1).fit(X[:,f_ind].reshape(-1, 1), y_class)\n",
    "    x_min, x_max = X[:,f_ind].min(), X[:,f_ind].max()\n",
    "    y_min, y_max = y.min(), y.max()\n",
    "    dx,dy = x_max-x_min,y_max-y_min\n",
    "    xx, yy = np.meshgrid(np.arange(x_min-0.04*dx, x_max+0.04*dx, plot_step),\n",
    "                         np.arange(y_min-0.04*dy, y_max+0.04*dy, plot_step))\n",
    "    \n",
    "    plt.figure(figsize=(4, 4))    \n",
    "    plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
    "\n",
    "    Z = dt_plt.predict(xx.ravel().reshape(-1, 1))\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cs = plt.contourf(xx, yy, Z, cmap=\"Pastel1\")#plt.cm.RdYlBu)\n",
    "    xpltlabel = X_labels[f_ind] + \"\\n\" + X_names[f_ind]\n",
    "\n",
    "    plt.xlabel(xpltlabel)\n",
    "    plt.ylabel(\"y\")\n",
    "\n",
    "    for i, color in zip(range(n_classes), plot_colors):\n",
    "        idx = np.where(y_class == i)\n",
    "        plt.scatter(X[idx, f_ind], y[idx], c=color,cmap=plt.cm.RdYlBu, edgecolor='black', s=15)    \n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation: Training/Test set split, Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T05:37:47.978618Z",
     "start_time": "2019-07-15T05:37:47.973825Z"
    }
   },
   "outputs": [],
   "source": [
    "# perform transformations on y\n",
    "y_orig = y.copy() # this is a backup of y\n",
    "\n",
    "# y = np.exp(y_orig)\n",
    "\n",
    "#log-transformation: either remove all samples with y=0 () or add a small amount to y to avoid log(0).\n",
    "# y = np.log(y+0.01)\n",
    "#or\n",
    "# y = np.log(y[y.nonzero()[0]])\n",
    "# y_labels_orig,X_orig = y_labels.copy(),X.copy()\n",
    "# y_labels = y_labels[y.nonzero()[0]]\n",
    "# X = X[y.nonzero()[0]]\n",
    "\n",
    "# y = abs(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove samples based on a feature-value\n",
    "# comment-out first line in Train/test split if using this\n",
    "select_feature = \"x78\"\n",
    "\n",
    "# define cutoff \n",
    "mask_prop = X[:,X_labels.index(select_feature)]<80\n",
    "\n",
    "X_sel,y_sel,y_labels_sel = X[mask_prop],y_cond[:,0][mask_prop],y_labels[mask_prop]\n",
    "print(\"Shape X: {}\".format(X_sel.shape))\n",
    "print(\"Shape y: {}\".format(y_sel.shape)) \n",
    "print(\"Shape labels: {}\".format(y_labels_sel.shape)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove samples based on index (0-indexed)\n",
    "# comment-out first line in Train/test split if using this\n",
    "\n",
    "exclude = [16,18,19,23,24,25]+[i for i in range(26,37)]\n",
    "print(exclude)\n",
    "mask = [i for i in range(len(y)) if i not in exclude]\n",
    "X_sel,y_sel,y_labels_sel = X[mask],y[mask],y_labels[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Test set split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:02:08.375756Z",
     "start_time": "2019-07-15T20:02:08.269464Z"
    }
   },
   "outputs": [],
   "source": [
    "# comment this line out if preselection was performed\n",
    "X_sel,y_sel,labels_sel,exclude = X,y,y_labels,[]\n",
    "\n",
    "# select method of split:\n",
    "# random\n",
    "# y_equidist - picks points that evenly span the output variable y. \n",
    "#              Normally doesn't pick highest/lowest values but this can be activated by changing the variable no_extrapolation in the respective section\n",
    "# ks - Kennard Stone algorithm picks points based on an even distriution in feature space\n",
    "# define - give a list of sample indices for either VS or TS in the corresponding code section \n",
    "# none - all samples in TS\n",
    "\n",
    "# the numbers in the variables VS and TS refer to the original 0-indexed sample numbers \n",
    "\n",
    "split = \"random\"#random\"\n",
    "test_ratio = 0.25\n",
    "\n",
    "if split == \"random\":\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_sel, y_sel, random_state=randomstate+3, test_size=test_ratio)    \n",
    "    TS = [np.argwhere(np.all(X==i,axis=1))[0,0] for i in X_train]\n",
    "    VS = [np.argwhere(np.all(X==i,axis=1))[0,0] for i in X_test]\n",
    "    \n",
    "elif split == \"define\":\n",
    "    # numbers according to sample lines in the excel sheet (that is, including indexes of 'excluded' samples)\n",
    "    # for defining the TS, change the names of TS and VS in the next three lines\n",
    "    TS = [1,2,3]\n",
    "    \n",
    "    TS = [i-1 for i in VS] # this can be commented out if 0-indexed numbers were defined above\n",
    "    VS = [i for i in range(X.shape[0]) if i not in VS and i not in exclude]\n",
    "    X_train, y_train,X_test, y_test = X[TS],y[TS],X[VS],y[VS]\n",
    "    \n",
    "elif split == \"ks\":\n",
    "    import kennardstonealgorithm as ks\n",
    "    TS,VS = ks.kennardstonealgorithm(X_sel,int((1-test_ratio)*np.shape(X_sel)[0]))\n",
    "    X_train, y_train,X_test, y_test = X_sel[TS], y_sel[TS],X_sel[VS], y_sel[VS]\n",
    "  \n",
    "    TS = [np.argwhere(np.all(X==i,axis=1))[0,0] for i in X_train]\n",
    "    VS = [np.argwhere(np.all(X==i,axis=1))[0,0] for i in X_test]   \n",
    "\n",
    "elif split == \"y_equidist\":\n",
    "    no_extrapolation = True\n",
    "    \n",
    "    import kennardstonealgorithm as ks\n",
    "    if no_extrapolation:\n",
    "        minmax = [np.argmin(y_sel),np.argmax(y_sel)]\n",
    "        y_ks = np.array(([i for i in y_sel if i not in [np.min(y_sel),np.max(y_sel)]]))\n",
    "        y_ks_indices = [i for i in range(len(y_sel)) if i not in minmax]\n",
    "        \n",
    "        # indices relative to y_ks:\n",
    "        VS_ks,TS_ks = ks.kennardstonealgorithm(y_ks.reshape(np.shape(y_ks)[0],1),int((test_ratio)*(2+np.shape(y_ks)[0])))\n",
    "        # indices relative to y_sel:\n",
    "        TS_ = sorted([y_ks_indices[i] for i in list(TS_ks)]+minmax)\n",
    "        VS_ = sorted([y_ks_indices[i] for i in VS_ks])\n",
    "\n",
    "    else:\n",
    "        VS_,TS_ = ks.kennardstonealgorithm(y_sel.reshape(np.shape(y_sel)[0],1),int((test_ratio)*np.shape(y_sel)[0]))\n",
    "    \n",
    "    X_train, y_train,X_test, y_test = X_sel[TS_], y_sel[TS_],X_sel[VS_], y_sel[VS_]\n",
    "    \n",
    "    # indices relative to y\n",
    "    TS = [np.argwhere(np.all(X==i,axis=1))[0,0] for i in X_train]\n",
    "    VS = [np.argwhere(np.all(X==i,axis=1))[0,0] for i in X_test]\n",
    "\n",
    "elif split == \"none\":\n",
    "    TS, VS = [i for i in range(X.shape[0]) if i not in exclude],[]\n",
    "    X_train, y_train,X_test, y_test = X[TS],y[TS],X[VS],y[VS]\n",
    "    \n",
    "else: \n",
    "    raise ValueError(\"split option not recognized\")\n",
    "     \n",
    "\n",
    "print(\"TS: {}\".format(TS))\n",
    "print(\"VS: {}\".format(VS))\n",
    "print(\"y_mean TS: {:.3f}\".format(np.mean(y_train)))\n",
    "print(\"y_mean VS: {:.3f}\".format(np.mean(y_test)))\n",
    "print(\"Shape X_train: {}\".format(X_train.shape))\n",
    "print(\"Shape X_test:  {}\".format(X_test.shape))   \n",
    "plt.figure(figsize=(5, 5))\n",
    "hist,bins = np.histogram(y_sel,bins=\"auto\")#\"auto\"\n",
    "plt.hist(y_train, bins, alpha=0.5, label='y_train',color=\"black\")\n",
    "plt.hist(y_test, bins, alpha=0.5, label='y_test')\n",
    "# plt.legend(loc='best')\n",
    "plt.xlabel(\"y\",fontsize=20)\n",
    "plt.ylabel(\"N samples\",fontsize=20)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:03:07.252006Z",
     "start_time": "2019-07-15T20:03:07.246283Z"
    }
   },
   "outputs": [],
   "source": [
    "# scale features by mean/variance, pick the relevant option (normally: StandardScaler)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# scaler = MinMaxScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_test_sc = scaler.transform(X_test)\n",
    "X_all_sc = scaler.transform(X_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-terms/Interaction terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T06:13:51.701344Z",
     "start_time": "2019-07-15T06:13:50.081688Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add polynomial features/interaction terms\n",
    "# this is not yet implemented properly in some sections. \n",
    "# for 5.1-manual selection: specify cross-term with space between the components: x1 x40 + x6\n",
    "# Essentially only section 5.2 can use cross-terms so far\n",
    "# don't run this twice\n",
    "\n",
    "polyfeats = PolynomialFeatures(degree=2,interaction_only=False,include_bias=False)\n",
    "X_train_p = polyfeats.fit_transform(X_train_sc)\n",
    "X_test_p = polyfeats.transform(X_test_sc)\n",
    "X_all_p = polyfeats.transform(X_all_sc)\n",
    "\n",
    "def add_to_x(matchobj):\n",
    "    if \"^\" in matchobj.group(0):\n",
    "        n = int(matchobj.group(0).split(\"^\")[0])+1\n",
    "        return(\"{} x{}\".format(n,n))\n",
    "    else:\n",
    "        return(str(int(matchobj.group(0))+1))\n",
    "    \n",
    "pfnames = np.asarray([re.sub(\"[0-9]+(\\^[0-9])*\",add_to_x,st) for st in polyfeats.get_feature_names()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out non-significant crossterms based on p-value with target variable\n",
    "p_val_cutoff = 0.005\n",
    "\n",
    "r2s = []\n",
    "pvals = []\n",
    "for f_ind,feature in enumerate(X_train_p.T):\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(feature, y_train)    \n",
    "    r2s.append(r_value**2)\n",
    "    pvals.append(p_value)\n",
    "    \n",
    "r2s_ = np.asarray(r2s)\n",
    "pvals_ = np.asarray(pvals)\n",
    "\n",
    "keep_p_ = [i[0] for i in np.argwhere(pvals_<p_val_cutoff) if i not in range(np.shape(X_all)[1])]\n",
    "keep_p = [i for i in range(np.shape(X_all)[1])] + keep_p_\n",
    "\n",
    "def sub_label_to_name(matchobj):\n",
    "    return(X_labelname_dict[matchobj.group(0)])\n",
    "def sub_labelname(matchobj):\n",
    "    return(matchobj.group(0)+\" \"+X_labelname_dict[matchobj.group(0)])\n",
    "    \n",
    "pfnames = np.asarray([re.sub(\"[0-9]+(\\^[0-9])*\",add_to_x,st) for st in polyfeats.get_feature_names()])\n",
    "X_p_labels = list(np.reshape(pfnames[keep_p],len(keep_p)))\n",
    "X_p_names = [re.sub(\"x[0-9]+\",sub_label_to_name,st) for st in X_p_labels]\n",
    "X_p_labelname = [re.sub(\"x[0-9]+\",sub_labelname,st) for st in X_p_labels]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# scaler = MinMaxScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train_p[:,keep_p])\n",
    "X_test_sc = scaler.transform(X_test_p[:,keep_p])\n",
    "X_all_sc = scaler.transform(X_all_p[:,keep_p])\n",
    "\n",
    "X_labels = X_p_labels\n",
    "X_names = X_p_names\n",
    "X_labelname = X_p_labelname\n",
    "\n",
    "print(\"{} cross-terms with p-value < {}\".format(len(keep_p_),p_val_cutoff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear modelling, feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual selection of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T21:34:50.963481Z",
     "start_time": "2019-07-15T21:34:50.737676Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#provide an x__ model (string, any order of terms)\n",
    "features_x = \"x134 + x5 + x54 + x17\" \n",
    "features_py = sorted([X_labels.index(i.strip()) for i in features_x.split(\"+\")])\n",
    "\n",
    "# features_py = []\n",
    "#features = sorted([int(i[1:]) for i in re.findall(\"x\\d+\",features_x)])\n",
    "#features_py = [i-1 for i in features]\n",
    "\n",
    "\n",
    "X_train_sel = X_train_sc[:,features_py]\n",
    "X_test_sel = X_test_sc[:,features_py]\n",
    "#lr = Ridge(alpha=1E-5).fit(X_train_sel, y_train)\n",
    "lr = LinearRegression().fit(X_train_sel, y_train)\n",
    "\n",
    "y_pred_train = lr.predict(X_train_sel)\n",
    "y_pred_test =  lr.predict(X_test_sel)\n",
    "q2,loo_train = loo.q2(X_train_sel,y_train)\n",
    "kfoldscores = repeated_k_fold(X_train_sel,y_train,k=4,n=200)\n",
    "\n",
    "print(\"Features: \" + \" + \".join([\"x\"+str(i+1) for i in sorted(features_py)]))\n",
    "print(\"\\nParameters:\\n{:10.4f} + \\n\".format(lr.intercept_) + \"\\n\".join([\"{:10.4f} * {}\".format(lr.coef_[i],X_labelname[sorted(features_py)[i]]) for i in range(len(features_py))]))\n",
    "\n",
    "print(f\"\\nTraining R2  = {lr.score(X_train_sel, y_train):.3f}\\nTraining Q2  = {q2:.3f}\")\n",
    "print(f\"Training MAE = {metrics.mean_absolute_error(y_train,y_pred_train):.3f}\")\n",
    "\n",
    "print(\"Training K-fold R2 = {:.3f} (+/- {:.3f})\".format(kfoldscores.mean(), kfoldscores.std() ** 2))\n",
    "print(f\"\\nTest R2      = {r2_val(y_test,y_pred_test,y_train):.3f}\\nTest MAE     = {metrics.mean_absolute_error(y_test,y_pred_test):.3f}\")\n",
    "\n",
    "plot_fit(y_train,y_pred_train,y_test,y_pred_test,leg=False,sav=False,label=\"y\",loo_pred=loo_train)\n",
    "\n",
    "model = sm.OLS(y_train, sm.add_constant(pd.DataFrame(X_train_sel))).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:06:29.632715Z",
     "start_time": "2019-07-15T20:06:28.781244Z"
    }
   },
   "outputs": [],
   "source": [
    "# add all individual features to the manual model\n",
    "add_df = pd.DataFrame(index=X_labelname,columns=[\"label_sep\",\"label_abs\",\"Training R2\",\"Training Q2\"],dtype=float)\n",
    "\n",
    "update_model = False\n",
    "for f_ind in range(len(X_labels)):\n",
    "    features_iter = features_py + [f_ind]    \n",
    "    X_train_sel = X_train_sc[:,features_iter]\n",
    "    X_test_sel = X_test_sc[:,features_iter]\n",
    "    # lr = Ridge(alpha=1E-1).fit(X_train_sel, y_train)\n",
    "    lr = LinearRegression().fit(X_train_sel, y_train)\n",
    "    q2,loo_train = loo.q2(X_train_sel,y_train,lr)\n",
    "    add_df.iloc[f_ind,:] = [X_labels[f_ind],\"x\"+str(f_ind+1),lr.score(X_train_sel, y_train),q2]\n",
    "\n",
    "if update_model and X_labelname.index(add_df['Training Q2'].idxmax()) not in features_py:\n",
    "    features_py.append(X_labelname.index(add_df['Training Q2'].idxmax()))\n",
    "add_df.sort_values(by=['Training Q2'],ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:07:29.114379Z",
     "start_time": "2019-07-15T20:07:29.075831Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove each individual feature from the manual model\n",
    "print(\"\\n\"+\" + \".join([\"x\"+str(i+1) for i in sorted(features_py)]))\n",
    "\n",
    "rem_df = pd.DataFrame(index=[X_labelname[i] for i in features_py],\n",
    "                      columns=[\"Training R2\",\"Training Q2\"])\n",
    "for f_ind in features_py:\n",
    "    features_iter = [i for i in features_py if i != f_ind]\n",
    "#     print(feature, x_names[f_ind])\n",
    "    X_train_sel = X_train_sc[:,features_iter]\n",
    "    X_test_sel = X_test_sc[:,features_iter]\n",
    "    # lr = Ridge(alpha=1E-1).fit(X_train_sel, y_train)\n",
    "    lr = LinearRegression().fit(X_train_sel, y_train)\n",
    "    q2,loo_train = loo.q2(X_train_sel,y_train,lr)\n",
    "    rem_df.loc[X_labelname[f_ind],:] = [lr.score(X_train_sel, y_train),q2]\n",
    "\n",
    "rem_df.sort_values(by=['Training Q2'],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward stepwise selection based on p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:08:59.679526Z",
     "start_time": "2019-07-15T20:08:56.859342Z"
    }
   },
   "outputs": [],
   "source": [
    "# Forward stepwise selection based on p-value\n",
    "# threshold values refer to p-value of individual features\n",
    "threshold_in = 0.05\n",
    "threshold_out = 0.075 # must be larger than threshold_in\n",
    "\n",
    "use_manual_feats = False # if True, model from previous section will be used as starting point\n",
    "\n",
    "import stepwise_selection2 as step_s\n",
    "if not use_manual_feats:\n",
    "    features_py=[]\n",
    "\n",
    "features_py = step_s.stepwise_selection(pd.DataFrame(X_train_sc), y_train,\n",
    "                    initial_list=features_py,threshold_in=threshold_in,threshold_out=threshold_out,verbose=True)\n",
    "\n",
    "print(\"\\n\"+\" + \".join([\"x\"+str(i+1) for i in sorted(features_py)]))\n",
    "print(\"\\n\"+\" + \".join([X_labelname[i] for i in sorted(features_py)]))\n",
    "\n",
    "X_train_sel = X_train_sc[:,features_py]\n",
    "X_test_sel = X_test_sc[:,features_py]\n",
    "#lr = Ridge(alpha=1E-6).fit(X_train_sel, y_train)\n",
    "lr = LinearRegression().fit(X_train_sel, y_train)\n",
    "y_pred_train = lr.predict(X_train_sel)\n",
    "y_pred_test =  lr.predict(X_test_sel)\n",
    "\n",
    "q2,loo_train = loo.q2(X_train_sel,y_train,LinearRegression())\n",
    "kfoldscores_self = repeated_k_fold(X_train_sel,y_train,k=5,n=100,reg=LinearRegression())\n",
    "\n",
    "print(\"Features: \" + \" + \".join([\"x\"+str(i+1) for i in sorted(features_py)]))\n",
    "print(\"\\nParameters:\\n{:10.4f} + \\n\".format(lr.intercept_) + \"\\n\".join([\"{:10.4f} * {}\".format(lr.coef_[i],X_labelname[sorted(features_py)[i]]) for i in range(len(features_py))]))\n",
    "\n",
    "print(f\"\\nTraining R2  = {lr.score(X_train_sel, y_train):.3f}\\nTraining Q2  = {q2:.3f}\")\n",
    "print(f\"Training MAE = {metrics.mean_absolute_error(y_train,y_pred_train):.3f}\")\n",
    "\n",
    "print(\"Training K-fold R2 = {:.3f} (+/- {:.3f})\".format(kfoldscores_self.mean(), kfoldscores_self.std() ** 2))\n",
    "print(f\"\\nTest R2      = {r2_val(y_test,y_pred_test,y_train):.3f}\\nTest MAE     = {metrics.mean_absolute_error(y_test,y_pred_test):.3f}\")\n",
    "\n",
    "plot_fit(y_train,y_pred_train,y_test,y_pred_test,leg=False,sav=False,label=\"y\",loo_pred=loo_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## other forward feature selection implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T06:25:13.462477Z",
     "start_time": "2019-07-15T06:25:13.360857Z"
    }
   },
   "outputs": [],
   "source": [
    "#Scikit-learn - forward feature selection\n",
    "# largely useless\n",
    "# options for criteria: \n",
    "# mutual_info_regression, f_regression\n",
    "# select number of features with k\n",
    "\n",
    "criteria = f_regression\n",
    "skb = SelectKBest(criteria,k=4).fit(X_train_sc,y_train)\n",
    "selected_feats = skb.get_support(indices=True)\n",
    "print(\" + \".join([\"x\"+str(i+1) for i in sorted(selected_feats)]))\n",
    "print(\"\\n\"+\" + \".join([X_names[i] for i in sorted(selected_feats)]))\n",
    "\n",
    "X_train_sel = skb.transform(X_train_sc)\n",
    "X_test_sel = skb.transform(X_test_sc)\n",
    "lr = LinearRegression().fit(X_train_sel, y_train)\n",
    "q2,loo_train = loo.q2(X_train_sel,y_train)\n",
    "\n",
    "y_pred_train = lr.predict(X_train_sel)\n",
    "y_pred_test =  lr.predict(X_test_sel)\n",
    "print(\"Training R2;Training Q2;Test R2;{:.2f};{:.2f};{:.2f}\".format(lr.score(X_train_sel, y_train),q2,r2_val(y_test,y_pred_test,y_train)))\n",
    "\n",
    "plot_fit(y_train,y_pred_train,y_test,y_pred_test)\n",
    "\n",
    "# uncomment to add model to candidate list\n",
    "# keepmodels.append(features_py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward stepwise selection based on AIC (\"aic\") or Q2 (\"q2\")\n",
    "criteria = \"aic\"\n",
    "\n",
    "import forwardselect_q5 as fsq\n",
    "\n",
    "df = pd.DataFrame(np.hstack((X_train_sc,y_train[:,None])))\n",
    "\n",
    "newcols = [\"x\"+str(i+1) for i in df.columns.values]\n",
    "df.columns = newcols\n",
    "response = newcols[-1]\n",
    "df.rename(columns={response:\"y\"},inplace=True)\n",
    "a,b = fsq.Forward_Select(df,\"y\",\"Regression\",criteria)\n",
    "selected_feats = [int(i[1:]) for i in b]\n",
    "print(\" + \".join([\"x\"+str(i+1) for i in sorted(selected_feats)]))\n",
    "print(\"\\n\"+\" + \".join([x_names[i] for i in sorted(selected_feats)]))\n",
    "\n",
    "X_train_sel = X_train_sc[:,selected_feats]\n",
    "X_test_sel = X_test_sc[:,selected_feats]\n",
    "lr = LinearRegression().fit(X_train_sel, y_train)\n",
    "q2,loo_train = loo.q2(X_train_sel,y_train)\n",
    "print(\"\\n\\n\")\n",
    "y_pred_train = lr.predict(X_train_sel)\n",
    "y_pred_test =  lr.predict(X_test_sel)\n",
    "print(\"Training R2;Training Q2;Test R2;{:.2f};{:.2f};{:.2f}\".format(lr.score(X_train_sel, y_train),q2,r2_val(y_test,y_pred_test,y_train)))\n",
    "\n",
    "plot_fit(y_train,y_pred_train,y_test,y_pred_test)\n",
    "\n",
    "# uncomment to add model to candidate list\n",
    "# keepmodels.append(features_py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward stepwise selection keeping a set of candidates at each step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T03:29:59.485960Z",
     "start_time": "2019-11-26T03:29:59.449406Z"
    }
   },
   "outputs": [],
   "source": [
    "# Forward stepwise selection keeping a set of candidates at each step\n",
    "n_steps = 2\n",
    "n_candidates = 10\n",
    "collin_criteria = 0.5 # this is R2\n",
    "skipfeatures = [] #[\"x4\",\"x3\"]\n",
    "\n",
    "import ForwardStepCandidates_2 as fsc\n",
    "df = pd.DataFrame(np.hstack((X_train_sc,y_train[:,None])))\n",
    "newcols = [\"x\"+str(i+1) for i in df.columns.values]\n",
    "df.columns = newcols\n",
    "response = newcols[-1]\n",
    "df.rename(columns={response:\"y\"},inplace=True)\n",
    "df.drop(skipfeatures,axis=1,inplace=True)\n",
    "\n",
    "results,models,scores,sortedmodels,candidates = fsc.ForwardStep_py(df,'y',\n",
    "                    n_steps=n_steps,n_candidates=n_candidates,collin_criteria=collin_criteria)\n",
    "model_sel = results.loc[0,\"Model\"]\n",
    "selected_feats = [X_labels.index(i) for i in models[model_sel].terms]\n",
    "X_train_sel = X_train_sc[:,selected_feats]\n",
    "X_test_sel = X_test_sc[:,selected_feats]\n",
    "print(\"\\n\\nBest model:\")\n",
    "print(models[model_sel].formula)\n",
    "print(\"1 + \"+\" + \".join([X_names[X_labels.index(i)] for i in models[candidates[0]].terms])+\"\\n\")\n",
    "lr = LinearRegression().fit(X_train_sel,y_train)\n",
    "y_pred_train = lr.predict(X_train_sel)\n",
    "y_pred_test =  lr.predict(X_test_sel)\n",
    "\n",
    "q2,loo_train = loo.q2(X_train_sel,y_train)\n",
    "kfoldscores_self = repeated_k_fold(X_train_sel,y_train,k=5,n=100)\n",
    "\n",
    "print(\"Features: \" + \" + \".join([\"x\"+str(i+1) for i in sorted(selected_feats)]))\n",
    "print(\"\\nParameters:\\n{:10.4f} + \\n\".format(lr.intercept_) + \"\\n\".join([\"{:10.4f} * {}\".format(lr.coef_[i],X_labelname[sorted(selected_feats)[i]]) for i in range(len(selected_feats))]))\n",
    "\n",
    "print(f\"\\nTraining R2  = {lr.score(X_train_sel, y_train):.3f}\\nTraining Q2  = {q2:.3f}\")\n",
    "print(f\"Training MAE = {metrics.mean_absolute_error(y_train,y_pred_train):.3f}\")\n",
    "\n",
    "print(\"Training K-fold R2 = {:.3f} (+/- {:.3f})\".format(kfoldscores_self.mean(), kfoldscores_self.std() ** 2))\n",
    "print(f\"\\nTest R2      = {r2_val(y_test,y_pred_test,y_train):.3f}\\nTest MAE     = {metrics.mean_absolute_error(y_test,y_pred_test):.3f}\")\n",
    "\n",
    "plot_fit(y_train,y_pred_train,y_test,y_pred_test,leg=False,sav=False,label=\"y\",loo_pred=loo_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View models as list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:18:16.132587Z",
     "start_time": "2019-07-15T20:18:16.119971Z"
    }
   },
   "outputs": [],
   "source": [
    "# view best models\n",
    "results.sort_values(by=['Q^2'],ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:19:39.136389Z",
     "start_time": "2019-07-15T20:19:39.127919Z"
    }
   },
   "outputs": [],
   "source": [
    "# view models with a specific number of terms\n",
    "selmods = results[results.n_terms <=1].sort_values(by=['Q^2'],ascending=False)\n",
    "selmods.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example for filtering results\n",
    "selmods2 = results.loc[[i for i in results.index if \"x113\" in results.loc[i,\"Model\"] and \"x49\" not in results.loc[i,\"Model\"]]][results.n_terms < 5].sort_values(by=['Q^2'],ascending=False)\n",
    "selmods2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T06:41:09.900600Z",
     "start_time": "2019-07-15T06:41:09.840203Z"
    }
   },
   "outputs": [],
   "source": [
    "# filter models that contain more than one term that is also in a reference model\n",
    "reference_model = 0 # this number refer to the index in 'results' or 'selmods', whichever is used \n",
    "use_df = results # or: selmods\n",
    "\n",
    "uniquemods = {use_df.loc[reference_model,\"Model\"]:reference_model}\n",
    "for ind in use_df.index:\n",
    "    selmod = use_df.loc[ind,\"Model\"]\n",
    "    if len(selmod) <= 2:\n",
    "        continue\n",
    "        \n",
    "    add = True\n",
    "    for mod in uniquemods.keys():\n",
    "        if len([i for i in mod if i in selmod]) >= 2:\n",
    "            add = False\n",
    "            break\n",
    "    if add:      \n",
    "        uniquemods[use_df.loc[ind,\"Model\"]] = ind\n",
    "    \n",
    "    \n",
    "print(len(uniquemods.keys()))\n",
    "selmods2 = results.loc[uniquemods.values()]\n",
    "selmods2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T21:35:38.965201Z",
     "start_time": "2019-07-15T21:35:38.717794Z"
    }
   },
   "outputs": [],
   "source": [
    "# visualize other models\n",
    "model_sel = results.loc[1,\"Model\"]\n",
    "\n",
    "#other ways of selecting models:\n",
    "# model_sel = results.iloc[selmods.index[3],0]\n",
    "# model_sel = results.iloc[785,0]\n",
    "# model_sel = (\"x100\",\"x31\")\n",
    "\n",
    "\n",
    "selected_feats = [X_labels.index(i) for i in models[model_sel].terms]\n",
    "X_train_sel = X_train_sc[:,selected_feats]\n",
    "X_test_sel = X_test_sc[:,selected_feats]\n",
    "print(models[model_sel].formula)\n",
    "print(\"1 + \"+\" + \".join([X_names[X_labels.index(i)] for i in models[model_sel].terms])+\"\\n\")\n",
    "lr = LinearRegression().fit(X_train_sel,y_train)\n",
    "y_pred_train = lr.predict(X_train_sel)\n",
    "y_pred_test =  lr.predict(X_test_sel)\n",
    "\n",
    "q2,loo_train = loo.q2(X_train_sel,y_train)\n",
    "kfoldscores_self = repeated_k_fold(X_train_sel,y_train,k=5,n=100)\n",
    "\n",
    "print(\"Features: \" + \" + \".join([\"x\"+str(i+1) for i in sorted(selected_feats)]))\n",
    "print(\"\\nParameters:\\n{:10.4f} + \\n\".format(lr.intercept_) + \"\\n\".join([\"{:10.4f} * {}\".format(lr.coef_[i],X_labelname[sorted(selected_feats)[i]]) for i in range(len(selected_feats))]))\n",
    "\n",
    "print(f\"\\nTraining R2  = {lr.score(X_train_sel, y_train):.3f}\\nTraining Q2  = {q2:.3f}\")\n",
    "print(f\"Training MAE = {metrics.mean_absolute_error(y_train,y_pred_train):.3f}\")\n",
    "\n",
    "print(\"Training K-fold R2 = {:.3f} (+/- {:.3f})\".format(kfoldscores_self.mean(), kfoldscores_self.std() ** 2))\n",
    "print(f\"\\nTest R2      = {r2_val(y_test,y_pred_test,y_train):.3f}\\nTest MAE     = {metrics.mean_absolute_error(y_test,y_pred_test):.3f}\")\n",
    "\n",
    "plot_fit(y_train,y_pred_train,y_test,y_pred_test,leg=False,sav=False,label=\"y\",loo_pred=loo_train)\n",
    "model = sm.OLS(y_train, sm.add_constant(pd.DataFrame(X_train_sel))).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression, no feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:25:05.331044Z",
     "start_time": "2019-07-15T20:25:05.153997Z"
    }
   },
   "outputs": [],
   "source": [
    "linm = Ridge().fit(X_train_sc, y_train)\n",
    "\n",
    "r2s = []\n",
    "q2s = []\n",
    "parms = []\n",
    "parm_range = np.logspace(-4,4,9)\n",
    "for parm in parm_range:\n",
    "    print(parm)\n",
    "    linm = Ridge(alpha=parm).fit(X_train_sc, y_train)\n",
    "    q2,loo_train = loo.q2(X_train_sc,y_train,Ridge(alpha=parm))\n",
    "    \n",
    "    y_pred_train = linm.predict(X_train_sc)\n",
    "    y_pred_test =  linm.predict(X_test_sc)\n",
    "    #print(\"Training R2;Training Q2;Test R2;{:.2f};{:.2f};{:.2f}\".format(lr.score(X_train_sel, y_train),q2,metrics.r2_score(y_pred_test,y_test)))\n",
    "    print(\"     Training R2;Training Q2;Test R2;{:.2f};{:.2f};{:.2f}\".format(linm.score(X_train_sc, y_train),q2,r2_val(y_test,y_pred_test,y_train)))\n",
    "    #print(lr.score(X_train_sel,y_train),lr.score(X_test_sel,y_test))\n",
    "        \n",
    "    r2s.append(linm.score(X_train_sc, y_train))\n",
    "    q2s.append(q2)\n",
    "    parms.append(parm)\n",
    "\n",
    "bestparm = parms[np.argmax(q2s)]\n",
    "print(\"\\n\\nUsing hyperparameter = {}\".format(bestparm))\n",
    "linm = Ridge(alpha=bestparm).fit(X_train_sc, y_train)\n",
    "y_pred_train = linm.predict(X_train_sc)\n",
    "y_pred_test =  linm.predict(X_test_sc)\n",
    "plot_fit(y_train,y_pred_train,y_test,y_pred_test)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:25:37.044047Z",
     "start_time": "2019-07-15T20:25:36.794175Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lasso feature selection\n",
    "\n",
    "r2s = []\n",
    "q2s = []\n",
    "parms = []\n",
    "parm_range = np.logspace(-4,1,6)\n",
    "for parm in parm_range:\n",
    "    print(parm)\n",
    "    lasso = Lasso(alpha=parm).fit(X_train_sc, y_train)\n",
    "    X_train_sel = X_train_sc[:,np.where(lasso.coef_!=0)[0]]\n",
    "    X_test_sel = X_test_sc[:,np.where(lasso.coef_!=0)[0]]\n",
    "    try:\n",
    "        q2,loo_train = loo.q2(X_train_sel,y_train,LinearRegression())\n",
    "        lr = LinearRegression().fit(X_train_sel,y_train)\n",
    "        y_pred_train = lr.predict(X_train_sel)\n",
    "        y_pred_test =  lr.predict(X_test_sel)\n",
    "        #print(\"Training R2;Training Q2;Test R2;{:.2f};{:.2f};{:.2f}\".format(lr.score(X_train_sel, y_train),q2,metrics.r2_score(y_pred_test,y_test)))\n",
    "        print(\"     Training R2;Training Q2;Test R2;{:.2f};{:.2f};{:.2f}\".format(lr.score(X_train_sel, y_train),q2,r2_val(y_test,y_pred_test,y_train)))\n",
    "        #print(lr.score(X_train_sel,y_train),lr.score(X_test_sel,y_test))\n",
    "    except:\n",
    "        pass\n",
    "    print(\"     Number of features used: {}\".format(np.sum(lasso.coef_ != 0)))\n",
    "    # print(np.where(lr.coef_ != 0)[1])\n",
    "    \n",
    "    r2s.append(lasso.score(X_train, y_train))\n",
    "    q2s.append(q2)\n",
    "    parms.append(parm)\n",
    "\n",
    "bestparm = parms[np.argmax(q2s)]\n",
    "print(\"\\n\\nUsing hyperparameter = {}\".format(bestparm))\n",
    "lasso = Lasso(alpha=bestparm).fit(X_train_sc, y_train)\n",
    "y_pred_train = lasso.predict(X_train_sc)\n",
    "y_pred_test =  lasso.predict(X_test_sc)\n",
    "plot_fit(y_train,y_pred_train,y_test,y_pred_test)    \n",
    "    \n",
    "# llbic = LassoLarsIC(criterion=\"bic\").fit(X_train_sc, y_train)\n",
    "# X_train_sel = X_train_sc[:,np.where(llbic.coef_!=0)[0]]\n",
    "# X_test_sel = X_test_sc[:,np.where(llbic.coef_!=0)[0]]\n",
    "# print(\"\\n\\nLassoLarsIC bic\")\n",
    "# try:\n",
    "#     q2,loo_train = loo.q2(X_train_sel,y_train,LinearRegression())\n",
    "#     print(\"Training R2;Training Q2;Test R2;{:.2f};{:.2f};{:.2f}\".format(llbic.score(X_train_sc, y_train),q2,llbic.score(X_test_sc, y_test)))\n",
    "# except:\n",
    "#     pass\n",
    "# print(\"Number of features used: {}\".format(np.sum(llbic.coef_ != 0)))\n",
    "\n",
    "# llaic = LassoLarsIC(criterion=\"aic\").fit(X_train_sc, y_train)\n",
    "# X_train_sel = X_train_sc[:,np.where(llaic.coef_!=0)[0]]\n",
    "# X_test_sel = X_test_sc[:,np.where(llaic.coef_!=0)[0]]\n",
    "# print(\"\\n\\nLassoLarsIC aic\")\n",
    "# try:\n",
    "#     q2,loo_train = loo.q2(X_train_sel,y_train,LinearRegression())\n",
    "#     print(\"Training R2;Training Q2;Test R2;{:.2f};{:.2f};{:.2f}\".format(llaic.score(X_train_sc, y_train),q2,llaic.score(X_test_sc, y_test)))\n",
    "# except:\n",
    "#     pass\n",
    "# print(\"Number of features used: {}\".format(np.sum(llaic.coef_ != 0)))\n",
    "\n",
    "# lassocv = LassoCV(cv=LeaveOneOut()).fit(X_train_sc, y_train)\n",
    "# X_train_sel = X_train_sc[:,np.where(lassocv.coef_!=0)[0]]\n",
    "# X_test_sel = X_test_sc[:,np.where(lassocv.coef_!=0)[0]]\n",
    "# q2,loo_train = loo.q2(X_train_sel,y_train,LinearRegression())\n",
    "# print(\"\\n\\nLassoCV\")\n",
    "# print(\"Training R2;Training Q2;Test R2;{:.2f};{:.2f};{:.2f}\".format(lassocv.score(X_train_sc, y_train),q2,lassocv.score(X_test_sc, y_test)))\n",
    "# print(\"Number of features used: {}\".format(np.sum(lassocv.coef_ != 0)))\n",
    "# print(np.where(lassocv.coef_ != 0)[1])\n",
    "\n",
    "llcv = LassoLarsCV(cv=LeaveOneOut()).fit(X_train_sc, y_train)\n",
    "X_train_sel = X_train_sc[:,np.where(llcv.coef_!=0)[0]]\n",
    "X_test_sel = X_test_sc[:,np.where(llcv.coef_!=0)[0]]\n",
    "q2,loo_train = loo.q2(X_train_sel,y_train,LinearRegression())\n",
    "print(\"\\n\\nLassoLarsCV\")\n",
    "print(\"Training R2;Training Q2;Test R2;{:.2f};{:.2f};{:.2f}\".format(llcv.score(X_train_sc, y_train),q2,r2_val(y_test,y_pred_test,y_train)))\n",
    "print(\"Number of features used: {}\".format(np.sum(llcv.coef_ != 0)))\n",
    "# print(np.where(llcv.coef_ != 0)[1])\n",
    "y_pred_train = llcv.predict(X_train_sc)\n",
    "y_pred_test =  llcv.predict(X_test_sc)\n",
    "plot_fit(y_train,y_pred_train,y_test,y_pred_test)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:26:15.400870Z",
     "start_time": "2019-07-15T20:26:15.080135Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "encv = ElasticNetCV(l1_ratio=[.01,.05,.66,.1,.2,.3, .5, .7, .9, .95, .99, 1],\n",
    "#                     n_alphas=500,\n",
    "                    alphas=np.logspace(-1,4,100),\n",
    "                    cv=3,n_jobs=-1,max_iter=1000000).fit(X_train_sc, y_train)\n",
    "X_train_sel = X_train_sc[:,np.where(encv.coef_!=0)[0]]\n",
    "X_test_sel = X_test_sc[:,np.where(encv.coef_!=0)[0]]\n",
    "q2,loo_train = loo.q2(X_train_sel,y_train,Ridge(alpha=encv.alpha_))\n",
    "print(\"\\n\\nElasticNetCV\")\n",
    "print(\"Training R2;Training Q2;Test R2;{:.2f};{:.2f};{:.2f}\".format(encv.score(X_train_sc, y_train),q2,r2_val(y_test,y_pred_test,y_train)))\n",
    "print(\"Number of features used: {}\".format(np.sum(encv.coef_ != 0)))\n",
    "print(\"Best hyperparameters: l1_ratio = {}, alpha = {}\".format(encv.l1_ratio_,encv.alpha_))\n",
    "\n",
    "y_pred_train = encv.predict(X_train_sc)\n",
    "y_pred_test =  encv.predict(X_test_sc)\n",
    "plot_fit(y_train,y_pred_train,y_test,y_pred_test) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orthogonal Matching Pursuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:27:01.940545Z",
     "start_time": "2019-07-15T20:27:01.866681Z"
    }
   },
   "outputs": [],
   "source": [
    "# Orthogonal Matching Pursuit\n",
    "\n",
    "parm_range = range(1,10)\n",
    "print(np.shape(X_train_sc))\n",
    "print(np.shape(y_train))\n",
    "for parm in parm_range:\n",
    "    print(parm)\n",
    "    omp = OrthogonalMatchingPursuit(n_nonzero_coefs=parm).fit(X_train_sc, y_train)\n",
    "    X_train_sel = X_train_sc[:,np.where(omp.coef_!=0)[0]]\n",
    "    X_test_sel = X_test_sc[:,np.where(omp.coef_!=0)[0]]\n",
    "    q2,loo_train = loo.q2(X_train_sel,y_train,LinearRegression())\n",
    "    print(\"     Training R2;Training Q2;Test R2;{:.2f};{:.2f};{:.2f}\".format(omp.score(X_train_sc, y_train),q2,r2_val(y_test,y_pred_test,y_train)))\n",
    "\n",
    "    print(\"     \"+\" + \".join([\"x\"+str(i+1) for i in np.where(omp.coef_ != 0)[0]]))\n",
    "    #print(\"     Number of features used: {}\".format(np.sum(lr.coef_ != 0)))\n",
    "    \n",
    "\n",
    "# ompcv = OrthogonalMatchingPursuitCV(cv=LeaveOneOut(),n_jobs=-1).fit(X_train_sc, y_train)\n",
    "# X_train_sel = X_train_sc[:,np.where(ompcv.coef_!=0)[0]]\n",
    "# X_test_sel = X_test_sc[:,np.where(ompcv.coef_!=0)[0]]\n",
    "# q2,loo_train = loo.q2(X_train_sel,y_train,LinearRegression())\n",
    "# print(\"\\n\\nOrthogonalMatchingPursuitCV\")\n",
    "# print(\"Training R2;Training Q2;Test R2;{:.2f};{:.2f};{:.2f}\".format(ompcv.score(X_train_sc, y_train),q2,ompcv.score(X_test_sc, y_test)))\n",
    "# print(\"Number of features used: {}\".format(np.sum(ompcv.coef_ != 0)))\n",
    "\n",
    "# print(\"\\n\"+\" + \".join([\"x\"+str(i+1) for i in np.where(ompcv.coef_!=0)[0]]))\n",
    "# print(\"\\n\"+\" + \".join([X_names[i] for i in np.where(ompcv.coef_!=0)[0]]))\n",
    "\n",
    "# y_pred_train = ompcv.predict(X_train_sc)\n",
    "# y_pred_test =  ompcv.predict(X_test_sc)\n",
    "# plot_fit(y_train,y_pred_train,y_test,y_pred_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Ridge Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:27:40.982970Z",
     "start_time": "2019-07-15T20:27:40.777173Z"
    }
   },
   "outputs": [],
   "source": [
    "# Kernel Ridge Regression \n",
    "# kernel options: linear, poly, rbf\n",
    "\n",
    "r2s = []\n",
    "q2s = []\n",
    "parms = []\n",
    "print(\"\\n\\nKernelRidge\")\n",
    "\n",
    "kernel=\"poly\"\n",
    "degree=2\n",
    "\n",
    "parm_range = np.logspace(-3,3,15)\n",
    "for parm in parm_range:\n",
    "    print(parm)\n",
    "    kr = KernelRidge(\n",
    "        kernel=kernel,\n",
    "        degree=degree,\n",
    "        alpha=parm\n",
    "    ).fit(X_train_sc, y_train)\n",
    "    q2,loo_train = loo.q2(X_train_sc,y_train,KernelRidge(kernel=kernel,degree=degree,alpha=parm))\n",
    "    print(\"     Training R2;Training Q2;Test R2;{:.2f};{:.2f};{:.2f}\".format(\n",
    "        kr.score(X_train_sc, y_train),q2,r2_val(y_test,y_pred_test,y_train)))\n",
    "    r2s.append(kr.score(X_train, y_train))\n",
    "    q2s.append(q2)\n",
    "    parms.append(parm)\n",
    "    \n",
    "bestparm = parms[np.argmax(q2s)]\n",
    "print(\"\\n\\nUsing hyperparameter = {}\".format(bestparm))\n",
    "kr = KernelRidge(\n",
    "        kernel=kernel,\n",
    "        degree=degree,\n",
    "        alpha=bestparm\n",
    "    ).fit(X_train_sc, y_train)\n",
    "y_pred_train = kr.predict(X_train_sc)\n",
    "y_pred_test =  kr.predict(X_test_sc)\n",
    "plot_fit(y_train,y_pred_train,y_test,y_pred_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T22:30:34.069356Z",
     "start_time": "2019-07-05T22:30:33.333775Z"
    }
   },
   "outputs": [],
   "source": [
    "# SVR\n",
    "\n",
    "r2s = []\n",
    "q2s = []\n",
    "parms = []\n",
    "print(\"\\n\\nSupport Vector Regression\")\n",
    "\n",
    "kernel=\"poly\"\n",
    "degree=2\n",
    "gamma=\"auto\"\n",
    "\n",
    "parm_range = np.logspace(-3,3,15)\n",
    "for parm in parm_range:\n",
    "    print(\"{:.2E}\".format(parm))\n",
    "    svr = SVR(\n",
    "        kernel=kernel,\n",
    "        degree=degree,\n",
    "        gamma=gamma,\n",
    "        C=parm\n",
    "    ).fit(X_train_sc, y_train)\n",
    "    \n",
    "    q2,loo_train = loo.q2(X_train_sc,y_train,SVR(kernel=kernel,degree=degree,gamma=gamma,C=parm))\n",
    "    print(\"     Training R2;Training Q2;Test R2;{:.2f};{:.2f};{:.2f}\".format(\n",
    "        svr.score(X_train_sc, y_train),q2,r2_val(y_test,y_pred_test,y_train)))\n",
    "    r2s.append(svr.score(X_train, y_train))\n",
    "    q2s.append(q2)\n",
    "    parms.append(parm)\n",
    "    \n",
    "bestparm = parms[np.argmax(q2s)]\n",
    "print(\"\\n\\nUsing hyperparameter = {}\".format(bestparm))\n",
    "svr = SVR(\n",
    "        kernel=kernel,\n",
    "        degree=degree,\n",
    "        gamma=gamma,\n",
    "        C=bestparm\n",
    "    ).fit(X_train_sc, y_train)\n",
    "y_pred_train = svr.predict(X_train_sc)\n",
    "y_pred_test =  svr.predict(X_test_sc)\n",
    "plot_fit(y_train,y_pred_train,y_test,y_pred_test)\n",
    "   \n",
    "\n",
    "# keepmodels_[svr] = ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression with Principal Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:28:41.087040Z",
     "start_time": "2019-07-15T20:28:40.283852Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Regression with Principal Components\n",
    "\n",
    "for npca in range(1,9):\n",
    "    pca = PCA(n_components=npca)\n",
    "    pca.fit(scaler.transform(X_sel))\n",
    "#     pca.fit(X_train_sc)\n",
    "    X_train_pca = pca.transform(X_train_sc)\n",
    "    X_test_pca = pca.transform(X_test_sc)\n",
    "\n",
    "    pca_score = pca.explained_variance_ratio_\n",
    "    pca_values = pca.singular_values_\n",
    "    V = pca.components_\n",
    "\n",
    "    linr = LinearRegression().fit(X_train_pca, y_train)\n",
    "#     linr = Ridge(alpha=1).fit(X_train_pca,y_train)\n",
    "    q2,loo_train = loo.q2(X_train_pca,y_train,LinearRegression())\n",
    "    print(\"\\n\\nPC Regression {}\".format(npca))\n",
    "    print(\"Training R2;Training Q2;Test R2;{:.2f};{:.2f};{:.2f}\".format(\n",
    "        linr.score(X_train_pca, y_train),q2,r2_val(y_test,y_pred_test,y_train)))\n",
    "\n",
    "    y_pred_train = linr.predict(X_train_pca)\n",
    "    y_pred_test =  linr.predict(X_test_pca)\n",
    "\n",
    "    plot_fit(y_train,y_pred_train,y_test,y_pred_test)\n",
    "\n",
    "    \n",
    "# # virtual screening\n",
    "# X_screen_sel = pca.transform(X_all_sc)\n",
    "# y_pred_vscreen = linr.predict(X_screen_sel)\n",
    "# y_vscreen[\"PCA5\"] = y_pred_vscreen\n",
    "\n",
    "\n",
    "# keepmodels_[PCA(n_components=2).fit(scaler.transform(X_sel))] = (\"n_components=2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T20:29:27.442564Z",
     "start_time": "2019-07-15T20:29:27.081106Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Random forest regression\n",
    "\n",
    "rf  = RandomForestRegressor(n_estimators=500,random_state=42,n_jobs=None,max_features=None,max_depth=None).fit(\n",
    "    X_train_sc, y_train)\n",
    "# rf = GradientBoostingRegressor(\n",
    "#     n_estimators=50,\n",
    "#     subsample=.6,\n",
    "#     max_depth=2,\n",
    "#     random_state=42,\n",
    "#     max_features=None,\n",
    "#     alpha=0.9,\n",
    "#     ).fit(X_train_sc, y_train)\n",
    "\n",
    "print(\"Training R2;Test R2;{:.2f};{:.2f}\".format(\n",
    "    rf.score(X_train_sc, y_train),r2_val(y_test,y_pred_test,y_train)))\n",
    "# print(np.where(llcv.coef_ != 0)[1])\n",
    "y_pred_train = rf.predict(X_train_sc)\n",
    "y_pred_test =  rf.predict(X_test_sc)\n",
    "plot_fit(y_train,y_pred_train,y_test,y_pred_test)   \n",
    "\n",
    "# keepmodels_[rf] = ()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "314px",
    "width": "313px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "325.02px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": "400"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 959.666666,
   "position": {
    "height": "981.263px",
    "left": "1534.33px",
    "right": "20px",
    "top": "5px",
    "width": "631.037px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
